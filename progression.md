# progression.md ‚Äî Build Log for My Virtual Machine
## This file is basically the VM‚Äôs diary. If something breaks, cries (me), explodes, or achieves sentience ‚Äî it‚Äôs going in here.

## Day 0: Setup and Goals
### Why I'm building a VM: 
Because... Why Not? üòÑ 
But on a serious note, the project I built before this was a fully-functioning shell, and that really got me hooked to systems. The next logical step is building a Virtual Machine.
And more importantly, because I just really love building stuff.
### What features I want:
I plan to be as masochistic as possible, so short answer: All of them.
Long answer:

#### Phase 1 ‚Äî Core VM (The Skeleton)
- A bytecode format (my VM‚Äôs own language, because why follow instructions?)
- Instruction dispatch loop (aka "the VM hamster wheel")
- A stack machine (push, pop, pray)
- Arithmetic ops (+, -, *, / ‚Äî but without floating point meltdowns)
- Control flow: jumps, conditionals, loops

#### Phase 2 ‚Äî Memory Model (Where objects go to live & occasionally die)
- Heap allocator
- Fixed layout for objects (integers, strings, arrays, etc.)
- Frames & call stack
- GC roots and tracing prep

#### Phase 3 ‚Äî Garbage Collector (The Grim Reaper)
- Mark & Sweep (basic)
- Correct root finding (global vars, stack frames)
- No accidental deletion of living objects (hopefully)
- No memory leaks when GC forgets to sweep (hopefully again)

#### Phase 4 ‚Äî Parser + Compiler (The VM learns to read)
- Tokenizer
- AST builder
- Compiler -> bytecode
- Functions, variables, local scopes
- Error messages that don‚Äôt gaslight users

#### Phase 5 ‚Äî High-Level Language Features (The spice)
I'll add this as I want:

##### Option A ‚Äî Performance Flex
- Bytecode optimizer
- Simple JIT (compile hot loops)
- Benchmark suite (compare with Lua/CPython)
- VM potentially gets faster than I do

##### Option B ‚Äî Developer Experience (the ‚Äúthis is actually fun to code in‚Äù route)
- Debugger (step, breakpoints, inspect stack/heap)
- Error messages with line numbers & context
- REPL with syntax highlighting

##### Option C ‚Äî Language Nerd Heaven (PL theory street cred)
- Closures with lexical scoping
- First-class functions
- Tail call optimization
- Theoretical bragging rights

### Languages/tools I'm using:
-C++
-VS Code
-Caffeine.
### What scares me/excites me:
Whatever scares me also excites me. üòà
### What I expect to learn:
Systems chops!
Also, how to function at 3 AM solely on caffeine fumes.

### Phase 0 (16/12/2025):
-Stack-based VM and not Register-based. Reason: fast enough, and much simpler to implement.

-Execution model:
  -The VM is a state machine whose state consists of the instruction pointer, operand stack and heap.
  -Instruction Pointer
  -Dispatch Loop (the heart of the VM)
  -Instruction Dispatch:
    -The first byte read is the opcode
    -The VM uses a giant switch statement to decode or dispatch the instruction.
    
-Value Model:
  -Dynamic typing
  -Value Type: single Value type with Tagged Union semantics.
  -Optimizations will not be implemented at this stage. Instead, I will design clean abstractions that allow future optimization.
  
-Memory Model:
  -Primitive values on the Vm stack; heap objects are referenced via pointers stored in Values.
-Non-goals: No GC, closures, JIT, threads, native FFI, Optimizer, Exceptions, YET. Will be implemented in later phases.
  
### Phase 1 (10/01/2026):
-Enum class Opcode for all opcodes: PUSH, POP, ADD, SUB, MUL, DIV, MOD, HALT. Why enum class and not enum? Because enum allows a lot of shady, implicit shit, and allowing implicit shit in your VM is the best way to shoot yourself in the foot. (For those who are curious about what I mean by 'shady shit': I mean implicit conversions and accidental misuse, which is dangerous in a VM, where explicitness and correctness are critical)

-Stack invariant followed as religiously as my daily call to my mom. :)

-The thing I had to figure out without which I could not have written a single line of code: The VM is a dumb little shit üôÑ. It does not (and rightly so) care about anything except the bytecode it's fed, and how to execute that bytecode in the most efficient manner possible.

-One bug I encountered that threw probably one of the longest error messages I've ever seen üíÄ: switch cases DO NOT create a new scope per case; the entire switch statement is one scope. If you don't care about what that means, just follow this: if you're declaring anything in your switch case, the switch case MUST be wrapped in {} brackets, especially if you're going to re-declare it in another case.<br>
Another bug that had me blaming the compiler for a while was enum casting. And guess what? It was due to my decision to be the good guy and pick enum class over enum. It would not have occured, had I gone with enum instead of enum class (because implicit conversion). :/ Can't get a break in this economy üôÑ


### Phase 2 (10/01/2026):
-Added proper function calls via CALL/RET and a separate call stack. Each function invocation gets its own call frame containing a return IP and a frame base into the operand stack. Operand stack holds values only; call stack holds control state only. Mixing the two is how you get early dementia.

-The key realization in this phase is this: A function call isn't wizardry. It's just dropping a 'brb I gotchu' bookmark and jumping. CALL saves where execution should resume and where the callee‚Äôs stack slice begins; RET restores both and nukes everything above the frame base. No magic, no hidden state, no 'function objects'.

-My sleep deprived self got bytecode and operand stack mixed up. üíÄ I spent 15 minutes trying to gaslight myself into committing a war crime: modifying the bytecode.

-Return values are handled explicitly: if the callee leaves a value on the stack, RET preserves it by popping it, cleaning the stack, and pushing it back. This also explains why stack cleanup is O(1): you‚Äôre not deleting values, you‚Äôre just rewinding the stack pointer.

-This phase went much smoother than Phase 1. That does NOT bode well üíÄ. Feels like the calm before the shitstorm.

### Phase 3 (12/01/2026):
Well... I wasn't wrong. That kinda was a shitstorm.

-Stack vs heap clicked pretty early: stack is only for handles to objects, while heap is for objects.

-The Value abstraction was pretty neat. A tagged union. This also incidentally happened to be the main phase shift my stubborn brain couldn't internalize: operand stack values are not integers anymore, they're struct Values, able to store any primitive data type and also references to heap objects. Arrays forced me to stop assuming that everything was an int and finally treat Value as a first-class runtime entity.

-Ran into a couple of segfaults that had me screaming inside in the library (don't do this shit for more than 5 hours a day, kids üôÑ). But segfaults were just undefined behaviour surfacing after invariants were violated. Can't even blame anything but my own saturated brain üò∂‚Äçüå´Ô∏è.

-I think this was the point I started feeling like I was building something solid.

### Phase 4 (13/01/2026):
GC was... Less of a nightmare than I'd thought. Or maybe I just finally got some good sleep yesterday.

-The main realisation in this stage was that the heap is a graph. Objects in the heap may point to other objects. For example, if the heap object is an array, some Value elements in it may be ValueType::OBJECT which point to another array, which in turn have elements that point to other objects, and so on. I hope that immediately smells like recursion to you, whoever's reading this. Therefore, **the mark phase is inherently recursive** (or graph-traversal based). Garbage collection is a DFS/BFS graph traversal.

-There **must** be strict separation between the mark phase and the sweep phase. If you mark during sweeping or sweep during marking... it's GGs bro. Garbage collection bugs like this rarely show up now. They slowly make you bleed more and more, until one day your VM can't function anymore.

-This was probably the phase that required the most discipline so far. As I mentioned earlier, garbage collection bugs are pernicious little fucktwats. They're like opps that wait until you're at rock bottom to strike.

Caveats:

-Memory allotted to the heap object payload is freed during GC, but the heap slot is not removed. This is because I use heap index as operand stack object reference. Slot reuse will be implemented in later phases.

-Garbage collection runs every time the number of objects on the heap exceeds 50. But what if the program actually needs more than 50 objects? The VM will end up running GC for every allotment after the 50th allotment. GC is costly: O(heapsize) complexity. A better system will be implemented in later phases.

### Phase 5 (I don't even remember the date bro not gonna lie)
I don't remember the date because I forgot to update this file when I completed that phase üíÄ. You can probably imagine, dear reader, how fucking DONE with this shit I was at that point. Well, anyway, I'm back now. I think.

This phase is where the project started going from 'glorified switch statement which gobbles up hardcoded bytecode and calls it a good day's work' to 'lowkey a serious language runtime with which you can compile and execute a program'.

The pipeline I decided on was:<br><br>
graph TD
    A[Source Code] --> B[Lexer <br/>characters to tokens]
    B --> C[Parser <br/>tokens to AST]
    C --> D[Compiler <br/>AST to bytecode]
    D --> E[VM <br/>bytecode to execution]

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#00ff0033,stroke:#333,stroke-width:2px
(that box I made around the pipeline architecture took me longer than I expected to make it :/)

This phase was pretty big, so it's best to split it into parts:
#### 5.1: Lexer (Characters ‚Üí Tokens)
Source code is NOT processed as words or whitespace-separated units, contrary to common belief. Instead, the program is a stream of characters. The lexer converts this into a stream of tokens.
<br>Each struct Token has a type (enum class TokenType in my code), lexeme (which is basically the original word in the program before it was converted to a token), and a line number.
<br>Example: let x = 5 processed to ‚Üí LET, IDENTIFIER(x), EQUAL, INTEGER(5), SEMICOLON.

<br>-The biggest insight in one line: 
'The lexer does not know what a variable or expression is. It only knows what characters belong together.'

<br><br>Some things that had me lowkey tweaking out in the library:
<br>-Off-by-one errors when advancing the index. THIS was the biggest one, and DAMN was it annoying. It's so deceptively easy to lose synchronicity in your VM. If I had just blindly forged on, it would've come back to bite me in my (well-muscled) butt in later stages. Luckily, my special brand of OCD does NOT let me move on, however much I just wanna die. (Google usually gives you the national helpline when you say shit like that, but my fellow systems people know exactly what I'm talking about)
<br>-peek() assertions failing at end-of-input.
<br>-Double incrementing index.
<br>-Misunderstanding when to advance vs when to inspect.

<br><br>Key lesson: 
Most lexer bugs are pointer movement bugs. Stupid thing slips through your hands faster than a ping-pog ball.

#### Stage 5.2 ‚Äî Parser (Tokens ‚Üí AST)
**The big conceptual shift.**
<br>The compiler is a lot dumber than I thought. Most of the logical work is done by the Parser. Just tokens is not enough, the compiler needs structure. That structure is thr Abstract Syntax Tree, which is constructed by the Parser.

<br><br>expression ‚Üí equality
<br>equality   ‚Üí comparison (== | != comparison)*
<br>comparison ‚Üí term (> < >= <= term)*
<br>term       ‚Üí factor (+ | - factor)*
<br>factor     ‚Üí unary (* / % unary)*
<br>unary      ‚Üí (! | -) unary | primary

<br><br>This shit was probably the most mind-blowing concept in the whole VM so far, so bear with me while I fangirl a bit. It's SO fucking neat how the recursion structure enforces precedence. When I first learnt this, I actually closed my laptop and just stared off into space, enjoying the insane dopamine rush from learning it. Some poor guy probably wondered why a random buffoon was giving him a thousand-yard stare in the library lol
Some other interesting things:
<br>-Reassigning left in Binary Expressions (to support chaining).
<br>-Wrapping everything in Expr (basically for the compiler to be able to hit Expr->compile(). Damn, is the compiler a lazy piece of shit. It's like Gilderoy Lockhart from Harry Potter-it has all the good rep despite all of its work having been done by others.)

<br><br>The AST is the last human-readable representation.

#### Stage 5.3 ‚Äî AST ‚Üí Bytecode Compilation
Compilation is just a tree walk. A postorder traversal of the AST, if you will.
<br>For example, for binary expressions:

<br>left->compile()
<br>right->compile()
<br>emit opcode

<br><br>This naturally produces stack-based bytecode:

<br><br>PUSH 1
<br>PUSH 2
<br>ADD

<br><br>-The AST itself defines the evaluation order.
<br>-Absolutely NO explicit VM opstack management in the compiler. The compiler doesn't even know that the stack exists, it just emits instructions in the logical order already determined by the AST.

<br><br>Some annoying stuff:
<br>-My caffeine-abused ass didn't realise I had to map TokenType ‚Üí Opcode. So I spent an embarrassing amount of time having midlife crises at 19 while my compiler emitted TokenType enumerations in the bytecode instead of Opcode ones. I screamed into my pillow (metaphorically, of course) when I realised.
<br>-Grouping expressions literally do nothing. They just forward compilation. Like bruh, stop wasting oxygen üôÑ

#### Stage 5.4 ‚Äî Statements (Expressions Were Not Enough)

This was a considerable mental shift, if not one of the biggest ones:
Expressions compute values. Statements do things.
<br>A program is not a list of expresssions. It's a list of statements.

<br>Parser changed from 
<br>parseExpression()
<br>to
<br>parseProgram() ‚Üí parseStatement().

<br><br>Statements are what we see when we zoom out. They're the smallest meaningful building block of a program.

#### Stage 5.5 ‚Äî Locals and Variable Slots

Variables are not stored by name at runtime. 
Instead:

<br>x ‚Üí slot 0
<br>y ‚Üí slot 1

<br><br>Compiler maintains:
<br>unordered_map<string, int> varSlots
<br>int nextLocalSlot

<br>VM only sees numeric slots.

<br><br>Important realization: The compiler resolves names. The VM only handles indices.
<br>This separation simplifies runtime execution enormously, and was something I kept botching up: mixing compiler responsibilities with VM behaviour.

<br><br>Bugs encountered
<br>-Missing global callframe (turns out, you have to push a global callframe at the beginning of the program, which made a horrible amount of sense), which incidentally lead to out of bounds local access ‚Üí segfaults.
<br>-SET_LOCAL popping stack twice
<br>-GET_LOCAL accessing empty locals array

This was where stack discipline became critical.

#### Stage 5.6 ‚Äî Control Flow (if / while)

This introduced the first non-linear execution. Although you might realise, whoever's reading this, that we did face a similar model earlier. In the CALl/RET phase. The difference here is, the program continues 

<br>Concept DLC unlocked: Backpatching
<br>When compiling, the structure is:
<br>if (cond) {...}, or while(cond){...}.
<br><br>So just compile the condition, push it onto the stack, emit a JUMP_IF_FALSE Opcode, and set the index to which you should jump to. Simple enough, right? Well, except you have no idea where you should jump to during compilation. So emit a random placeholder index, compile the block, and then revisit it to change it. This is what all compilers do.

<br><br>Major sources of confusion/crashes:
<br>-IfStmt must include both the condition AND the Block to be executed if the condition is true.
<br>-Each IfStmt MUST deal with its own compilation. State cannot be globally maintained, because nested blocks can and will overwrite state and cause chaos.
<br>-Not incrementing IP in the VM
<br>-Stack not cleaned after conditions
<br>-Stack cleaned twice after conditions (yes, really. I have moments of profound idiocy, and that shouldn't be surprising.)
<br>-The compiler should NEVER perform arithmetic while emitting/patching jump index. At any point, it should only read the size of the bytecode so far, so be careful in where you do that.

<br><br>I think the biggest and most elusive change in this phase was psychological: Earlier phases felt concept-heavy but straightforward. Phase 5 required trusting abstractions:

<br><br>Lexer doesn‚Äôt understand meaning.
<br>Parser doesn‚Äôt execute.
<br>Compiler doesn‚Äôt store values.
<br>VM doesn‚Äôt know variable names.
<br>Each layer does one thing only.

<br><br>Once this separation clicked, development accelerated dramatically, although the code was the most boring and mechanical (but extensive-my code more than doubled in line count in this stage alone) so far. But mixing the layers' responsibilities was definitely what caused the most pain in this phase. You have to trust what you're building; trust the separation of responsibilities between the layers.

<br><br>But okay so basically (hear me out üíÄ) we ripped the source code apart character-by-character, made tokens from it, assembled those tokens into expressions, and wrapped expressions into statements back again. Kinda retarded, or so I thought when the big picture first became clear to me. But thinking about it like this helped:
<br><br>**Why This Architecture Exists (In Practice):**
<br>At first glance, the pipeline looks unnecessarily complicated. Why not interpret source code directly?
Because each stage removes one kind of complexity:

<br><br>Lexer removes character-level ambiguity.
<br>Parser removes syntactic ambiguity.
<br>Compiler removes structural complexity.
<br>VM executes only simple instructions.

<br><br>Each layer converts a complex problem into a simpler one for the next layer.
Without this separation, every part of the system would need to understand everything else.<br><br>
